{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Sentiment of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this analysis is to predict if a review rates the movie positively or negatively. Inside this dataset there are 25,000 labeled movies reviews for training, 50,000 unlabeled reviews for training, and 25,000 reviews for testing. More information about the data can be found at: https://www.kaggle.com/c/word2vec-nlp-tutorial.\n",
    "\n",
    "This data comes from the 2015 Kaggle competition, \"Bag of Words Meets Bags of Popcorn.\" Despite the competition being finished, I thought it could still serve as a useful tool for my first Natural Lanugage Processing (NLP) project. Within this analysis you will find two methods for predicting the sentiment of movie reviews. I wanted to experiment with a couple of strategies to gain an understanding of different options and compare their results. The two methods that I will use are: \n",
    "- Bag of Centroids with word2vec\n",
    "- TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk.data\n",
    "import logging  \n",
    "import multiprocessing\n",
    "import time\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.ensemble import VotingClassifier as vc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", \n",
    "                    header=0, \n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", \n",
    "                              header=0, \n",
    "                              delimiter=\"\\t\", \n",
    "                              quoting=3 )\n",
    "\n",
    "test = pd.read_csv(\"testData.tsv\", \n",
    "                   header=0, \n",
    "                   delimiter=\"\\t\", \n",
    "                   quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(50000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compare the lengths of the datasets\n",
    "print(train.shape)\n",
    "print(unlabeled_train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at each of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Naturally in a film who\\'s main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. However there is a craftsmanship and completeness to the film which anyone can enjoy. The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce\\'s short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good with the data. No surprises so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1: Bag of Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords = True, stem_words = True):\n",
    "    # Clean the text, with the option to remove stopwords and stem words.\n",
    "\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    review_text = review_text.lower()\n",
    "\n",
    "    # Optionally remove stop words (true by default)\n",
    "    if remove_stopwords:\n",
    "        words = review_text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        review_text = \" \".join(words)\n",
    "\n",
    "    # Clean the text\n",
    "    review_text = re.sub(r\"[^A-Za-z0-9!?\\'\\`]\", \" \", review_text)\n",
    "    review_text = re.sub(r\"it's\", \" it is\", review_text)\n",
    "    review_text = re.sub(r\"that's\", \" that is\", review_text)\n",
    "    review_text = re.sub(r\"\\'s\", \" 's\", review_text)\n",
    "    review_text = re.sub(r\"\\'ve\", \" have\", review_text)\n",
    "    review_text = re.sub(r\"won't\", \" will not\", review_text)\n",
    "    review_text = re.sub(r\"don't\", \" do not\", review_text)\n",
    "    review_text = re.sub(r\"can't\", \" can not\", review_text)\n",
    "    review_text = re.sub(r\"cannot\", \" can not\", review_text)\n",
    "    review_text = re.sub(r\"n\\'t\", \" n\\'t\", review_text)\n",
    "    review_text = re.sub(r\"\\'re\", \" are\", review_text)\n",
    "    review_text = re.sub(r\"\\'d\", \" would\", review_text)\n",
    "    review_text = re.sub(r\"\\'ll\", \" will\", review_text)\n",
    "    review_text = re.sub(r\"!\", \" ! \", review_text)\n",
    "    review_text = re.sub(r\"\\?\", \" ? \", review_text)\n",
    "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
    "    \n",
    "    # Shorten words to their stems\n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        review_text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words, with each word as its own string\n",
    "    return review_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    # Split a review into parsed sentences\n",
    "    # Returns a list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    # Use the NLTK tokenizer to split the review into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence))\n",
    "    \n",
    "    # Return the list of sentences\n",
    "    # Each sentence is a list of words, so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "//anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = [] \n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print (\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "\n",
      "['with', 'stuff', 'go', 'moment', 'mj', 'i', 'have', 'start', 'listen', 'music', 'watch', 'odd', 'documentari', 'there', 'watch', 'wiz', 'watch', 'moonwalk', 'again']\n",
      "\n",
      "['mayb', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'realli', 'cool', 'eighti', 'mayb', 'make', 'mind', 'whether', 'guilti', 'innoc']\n"
     ]
    }
   ],
   "source": [
    "# Check how many sentences we have in total \n",
    "print (len(sentences))\n",
    "print()\n",
    "print (sentences[0])\n",
    "print()\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 17:09:08,042 : INFO : collecting all words and their counts\n",
      "2017-03-13 17:09:08,043 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-13 17:09:08,078 : INFO : PROGRESS: at sentence #10000, processed 131544 words, keeping 12784 word types\n",
      "2017-03-13 17:09:08,132 : INFO : PROGRESS: at sentence #20000, processed 262182 words, keeping 17571 word types\n",
      "2017-03-13 17:09:08,174 : INFO : PROGRESS: at sentence #30000, processed 388992 words, keeping 20988 word types\n",
      "2017-03-13 17:09:08,211 : INFO : PROGRESS: at sentence #40000, processed 520101 words, keeping 23883 word types\n",
      "2017-03-13 17:09:08,246 : INFO : PROGRESS: at sentence #50000, processed 647817 words, keeping 26225 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 17:09:08,283 : INFO : PROGRESS: at sentence #60000, processed 775677 words, keeping 28215 word types\n",
      "2017-03-13 17:09:08,331 : INFO : PROGRESS: at sentence #70000, processed 904602 words, keeping 30054 word types\n",
      "2017-03-13 17:09:08,367 : INFO : PROGRESS: at sentence #80000, processed 1031248 words, keeping 31698 word types\n",
      "2017-03-13 17:09:08,416 : INFO : PROGRESS: at sentence #90000, processed 1162109 words, keeping 33415 word types\n",
      "2017-03-13 17:09:08,452 : INFO : PROGRESS: at sentence #100000, processed 1290091 words, keeping 34868 word types\n",
      "2017-03-13 17:09:08,488 : INFO : PROGRESS: at sentence #110000, processed 1417840 words, keeping 36191 word types\n",
      "2017-03-13 17:09:08,527 : INFO : PROGRESS: at sentence #120000, processed 1546128 words, keeping 37632 word types\n",
      "2017-03-13 17:09:08,569 : INFO : PROGRESS: at sentence #130000, processed 1677040 words, keeping 38899 word types\n",
      "2017-03-13 17:09:08,606 : INFO : PROGRESS: at sentence #140000, processed 1800051 words, keeping 39985 word types\n",
      "2017-03-13 17:09:08,645 : INFO : PROGRESS: at sentence #150000, processed 1930522 words, keeping 41231 word types\n",
      "2017-03-13 17:09:08,689 : INFO : PROGRESS: at sentence #160000, processed 2059584 words, keeping 42377 word types\n",
      "2017-03-13 17:09:08,735 : INFO : PROGRESS: at sentence #170000, processed 2188907 words, keeping 43437 word types\n",
      "2017-03-13 17:09:08,780 : INFO : PROGRESS: at sentence #180000, processed 2316238 words, keeping 44490 word types\n",
      "2017-03-13 17:09:08,832 : INFO : PROGRESS: at sentence #190000, processed 2446419 words, keeping 45435 word types\n",
      "2017-03-13 17:09:08,884 : INFO : PROGRESS: at sentence #200000, processed 2576169 words, keeping 46332 word types\n",
      "2017-03-13 17:09:08,928 : INFO : PROGRESS: at sentence #210000, processed 2704663 words, keeping 47298 word types\n",
      "2017-03-13 17:09:09,000 : INFO : PROGRESS: at sentence #220000, processed 2834638 words, keeping 48284 word types\n",
      "2017-03-13 17:09:09,047 : INFO : PROGRESS: at sentence #230000, processed 2963269 words, keeping 49187 word types\n",
      "2017-03-13 17:09:09,097 : INFO : PROGRESS: at sentence #240000, processed 3094568 words, keeping 50120 word types\n",
      "2017-03-13 17:09:09,140 : INFO : PROGRESS: at sentence #250000, processed 3218844 words, keeping 51017 word types\n",
      "2017-03-13 17:09:09,181 : INFO : PROGRESS: at sentence #260000, processed 3346557 words, keeping 51839 word types\n",
      "2017-03-13 17:09:09,223 : INFO : PROGRESS: at sentence #270000, processed 3474790 words, keeping 52856 word types\n",
      "2017-03-13 17:09:09,265 : INFO : PROGRESS: at sentence #280000, processed 3605436 words, keeping 54122 word types\n",
      "2017-03-13 17:09:09,303 : INFO : PROGRESS: at sentence #290000, processed 3734919 words, keeping 55287 word types\n",
      "2017-03-13 17:09:09,351 : INFO : PROGRESS: at sentence #300000, processed 3865108 words, keeping 56361 word types\n",
      "2017-03-13 17:09:09,393 : INFO : PROGRESS: at sentence #310000, processed 3995801 words, keeping 57403 word types\n",
      "2017-03-13 17:09:09,435 : INFO : PROGRESS: at sentence #320000, processed 4126090 words, keeping 58508 word types\n",
      "2017-03-13 17:09:09,478 : INFO : PROGRESS: at sentence #330000, processed 4254459 words, keeping 59496 word types\n",
      "2017-03-13 17:09:09,521 : INFO : PROGRESS: at sentence #340000, processed 4388087 words, keeping 60491 word types\n",
      "2017-03-13 17:09:09,562 : INFO : PROGRESS: at sentence #350000, processed 4517176 words, keeping 61403 word types\n",
      "2017-03-13 17:09:09,604 : INFO : PROGRESS: at sentence #360000, processed 4645072 words, keeping 62320 word types\n",
      "2017-03-13 17:09:09,662 : INFO : PROGRESS: at sentence #370000, processed 4776366 words, keeping 63179 word types\n",
      "2017-03-13 17:09:09,716 : INFO : PROGRESS: at sentence #380000, processed 4907028 words, keeping 64129 word types\n",
      "2017-03-13 17:09:09,806 : INFO : PROGRESS: at sentence #390000, processed 5040339 words, keeping 64943 word types\n",
      "2017-03-13 17:09:09,848 : INFO : PROGRESS: at sentence #400000, processed 5169273 words, keeping 65736 word types\n",
      "2017-03-13 17:09:09,896 : INFO : PROGRESS: at sentence #410000, processed 5297521 words, keeping 66498 word types\n",
      "2017-03-13 17:09:09,936 : INFO : PROGRESS: at sentence #420000, processed 5426239 words, keeping 67307 word types\n",
      "2017-03-13 17:09:09,981 : INFO : PROGRESS: at sentence #430000, processed 5558239 words, keeping 68119 word types\n",
      "2017-03-13 17:09:10,036 : INFO : PROGRESS: at sentence #440000, processed 5689772 words, keeping 68917 word types\n",
      "2017-03-13 17:09:10,086 : INFO : PROGRESS: at sentence #450000, processed 5818743 words, keeping 69815 word types\n",
      "2017-03-13 17:09:10,135 : INFO : PROGRESS: at sentence #460000, processed 5953804 words, keeping 70639 word types\n",
      "2017-03-13 17:09:10,180 : INFO : PROGRESS: at sentence #470000, processed 6085777 words, keeping 71314 word types\n",
      "2017-03-13 17:09:10,220 : INFO : PROGRESS: at sentence #480000, processed 6213275 words, keeping 72102 word types\n",
      "2017-03-13 17:09:10,255 : INFO : PROGRESS: at sentence #490000, processed 6345243 words, keeping 72966 word types\n",
      "2017-03-13 17:09:10,300 : INFO : PROGRESS: at sentence #500000, processed 6473523 words, keeping 73648 word types\n",
      "2017-03-13 17:09:10,346 : INFO : PROGRESS: at sentence #510000, processed 6604272 words, keeping 74402 word types\n",
      "2017-03-13 17:09:10,382 : INFO : PROGRESS: at sentence #520000, processed 6733745 words, keeping 75128 word types\n",
      "2017-03-13 17:09:10,425 : INFO : PROGRESS: at sentence #530000, processed 6863747 words, keeping 75793 word types\n",
      "2017-03-13 17:09:10,466 : INFO : PROGRESS: at sentence #540000, processed 6994166 words, keeping 76508 word types\n",
      "2017-03-13 17:09:10,504 : INFO : PROGRESS: at sentence #550000, processed 7124768 words, keeping 77222 word types\n",
      "2017-03-13 17:09:10,541 : INFO : PROGRESS: at sentence #560000, processed 7253349 words, keeping 77908 word types\n",
      "2017-03-13 17:09:10,577 : INFO : PROGRESS: at sentence #570000, processed 7385880 words, keeping 78512 word types\n",
      "2017-03-13 17:09:10,624 : INFO : PROGRESS: at sentence #580000, processed 7514021 words, keeping 79205 word types\n",
      "2017-03-13 17:09:10,678 : INFO : PROGRESS: at sentence #590000, processed 7644579 words, keeping 79903 word types\n",
      "2017-03-13 17:09:10,732 : INFO : PROGRESS: at sentence #600000, processed 7773421 words, keeping 80515 word types\n",
      "2017-03-13 17:09:10,781 : INFO : PROGRESS: at sentence #610000, processed 7901736 words, keeping 81245 word types\n",
      "2017-03-13 17:09:10,833 : INFO : PROGRESS: at sentence #620000, processed 8033406 words, keeping 81848 word types\n",
      "2017-03-13 17:09:10,883 : INFO : PROGRESS: at sentence #630000, processed 8163099 words, keeping 82462 word types\n",
      "2017-03-13 17:09:10,943 : INFO : PROGRESS: at sentence #640000, processed 8290878 words, keeping 83131 word types\n",
      "2017-03-13 17:09:10,994 : INFO : PROGRESS: at sentence #650000, processed 8422487 words, keeping 83786 word types\n",
      "2017-03-13 17:09:11,045 : INFO : PROGRESS: at sentence #660000, processed 8551421 words, keeping 84407 word types\n",
      "2017-03-13 17:09:11,088 : INFO : PROGRESS: at sentence #670000, processed 8680148 words, keeping 84962 word types\n",
      "2017-03-13 17:09:11,132 : INFO : PROGRESS: at sentence #680000, processed 8811073 words, keeping 85535 word types\n",
      "2017-03-13 17:09:11,180 : INFO : PROGRESS: at sentence #690000, processed 8940059 words, keeping 86150 word types\n",
      "2017-03-13 17:09:11,214 : INFO : PROGRESS: at sentence #700000, processed 9072146 words, keeping 86779 word types\n",
      "2017-03-13 17:09:11,259 : INFO : PROGRESS: at sentence #710000, processed 9201289 words, keeping 87289 word types\n",
      "2017-03-13 17:09:11,303 : INFO : PROGRESS: at sentence #720000, processed 9331911 words, keeping 87798 word types\n",
      "2017-03-13 17:09:11,342 : INFO : PROGRESS: at sentence #730000, processed 9463030 words, keeping 88376 word types\n",
      "2017-03-13 17:09:11,381 : INFO : PROGRESS: at sentence #740000, processed 9590841 words, keeping 88960 word types\n",
      "2017-03-13 17:09:11,418 : INFO : PROGRESS: at sentence #750000, processed 9717765 words, keeping 89470 word types\n",
      "2017-03-13 17:09:11,457 : INFO : PROGRESS: at sentence #760000, processed 9844161 words, keeping 89991 word types\n",
      "2017-03-13 17:09:11,507 : INFO : PROGRESS: at sentence #770000, processed 9975848 words, keeping 90604 word types\n",
      "2017-03-13 17:09:11,560 : INFO : PROGRESS: at sentence #780000, processed 10109141 words, keeping 91168 word types\n",
      "2017-03-13 17:09:11,613 : INFO : PROGRESS: at sentence #790000, processed 10240794 words, keeping 91711 word types\n",
      "2017-03-13 17:09:11,641 : INFO : collected 92061 word types from a corpus of 10312281 raw words and 795538 sentences\n",
      "2017-03-13 17:09:11,642 : INFO : Loading a fresh vocabulary\n",
      "2017-03-13 17:09:11,819 : INFO : min_count=5 retains 32269 unique words (35% of original 92061, drops 59792)\n",
      "2017-03-13 17:09:11,820 : INFO : min_count=5 leaves 10219973 word corpus (99% of original 10312281, drops 92308)\n",
      "2017-03-13 17:09:12,009 : INFO : deleting the raw counts dictionary of 92061 items\n",
      "2017-03-13 17:09:12,014 : INFO : sample=0.0001 downsamples 695 most-common words\n",
      "2017-03-13 17:09:12,015 : INFO : downsampling leaves estimated 6420755 word corpus (62.8% of prior 10219973)\n",
      "2017-03-13 17:09:12,016 : INFO : estimated required memory for 32269 words and 300 dimensions: 93580100 bytes\n",
      "2017-03-13 17:09:12,173 : INFO : resetting layer weights\n",
      "2017-03-13 17:09:13,097 : INFO : training model with 1 workers on 32269 vocabulary and 300 features, using sg=0 hs=0 sample=0.0001 negative=5 window=20\n",
      "2017-03-13 17:09:13,098 : INFO : expecting 795538 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-13 17:09:14,172 : INFO : PROGRESS: at 0.38% examples, 120363 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:15,229 : INFO : PROGRESS: at 0.83% examples, 127782 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:16,252 : INFO : PROGRESS: at 1.24% examples, 127861 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:17,290 : INFO : PROGRESS: at 1.77% examples, 136167 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:18,301 : INFO : PROGRESS: at 2.18% examples, 134861 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:19,311 : INFO : PROGRESS: at 2.69% examples, 138844 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:20,338 : INFO : PROGRESS: at 3.36% examples, 148435 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:21,374 : INFO : PROGRESS: at 3.90% examples, 150839 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:22,413 : INFO : PROGRESS: at 4.47% examples, 153262 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:23,434 : INFO : PROGRESS: at 5.01% examples, 154991 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:24,452 : INFO : PROGRESS: at 5.59% examples, 157526 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:25,454 : INFO : PROGRESS: at 6.10% examples, 157796 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:26,474 : INFO : PROGRESS: at 6.66% examples, 158742 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:27,504 : INFO : PROGRESS: at 7.10% examples, 157344 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:28,512 : INFO : PROGRESS: at 7.60% examples, 157509 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:29,528 : INFO : PROGRESS: at 8.16% examples, 158759 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:09:30,535 : INFO : PROGRESS: at 8.72% examples, 159922 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:31,550 : INFO : PROGRESS: at 9.14% examples, 158539 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:32,578 : INFO : PROGRESS: at 9.62% examples, 158161 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:09:33,628 : INFO : PROGRESS: at 10.03% examples, 156445 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:34,730 : INFO : PROGRESS: at 10.34% examples, 153062 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:09:35,734 : INFO : PROGRESS: at 10.78% examples, 152609 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:36,761 : INFO : PROGRESS: at 11.38% examples, 154150 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:37,763 : INFO : PROGRESS: at 12.03% examples, 156511 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:38,778 : INFO : PROGRESS: at 12.75% examples, 159323 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:39,806 : INFO : PROGRESS: at 13.42% examples, 161328 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:40,810 : INFO : PROGRESS: at 14.06% examples, 162892 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:09:41,862 : INFO : PROGRESS: at 14.58% examples, 162780 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:42,875 : INFO : PROGRESS: at 14.96% examples, 161436 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:43,897 : INFO : PROGRESS: at 15.41% examples, 160720 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:44,944 : INFO : PROGRESS: at 15.76% examples, 158922 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:45,949 : INFO : PROGRESS: at 16.22% examples, 158617 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:46,976 : INFO : PROGRESS: at 16.79% examples, 159123 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:48,003 : INFO : PROGRESS: at 17.35% examples, 159631 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:49,008 : INFO : PROGRESS: at 17.92% examples, 160377 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:50,045 : INFO : PROGRESS: at 18.49% examples, 160751 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:51,071 : INFO : PROGRESS: at 18.93% examples, 160177 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:52,118 : INFO : PROGRESS: at 19.38% examples, 159545 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:53,141 : INFO : PROGRESS: at 19.74% examples, 158428 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:54,159 : INFO : PROGRESS: at 20.20% examples, 158147 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:55,177 : INFO : PROGRESS: at 20.63% examples, 157577 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:56,189 : INFO : PROGRESS: at 20.88% examples, 155751 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:57,201 : INFO : PROGRESS: at 21.51% examples, 156696 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:58,222 : INFO : PROGRESS: at 22.13% examples, 157558 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:09:59,241 : INFO : PROGRESS: at 22.74% examples, 158240 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:00,245 : INFO : PROGRESS: at 23.43% examples, 159478 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:01,262 : INFO : PROGRESS: at 24.13% examples, 160749 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:02,296 : INFO : PROGRESS: at 24.75% examples, 161395 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:03,353 : INFO : PROGRESS: at 25.24% examples, 161089 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:04,379 : INFO : PROGRESS: at 25.51% examples, 159558 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:05,422 : INFO : PROGRESS: at 25.99% examples, 159344 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:06,430 : INFO : PROGRESS: at 26.32% examples, 158192 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:10:07,430 : INFO : PROGRESS: at 26.82% examples, 158237 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:08,457 : INFO : PROGRESS: at 27.50% examples, 159233 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:09,461 : INFO : PROGRESS: at 28.17% examples, 160264 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:10,476 : INFO : PROGRESS: at 28.85% examples, 161215 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:11,477 : INFO : PROGRESS: at 29.54% examples, 162296 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:12,505 : INFO : PROGRESS: at 30.22% examples, 163148 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:13,507 : INFO : PROGRESS: at 30.83% examples, 163742 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:14,508 : INFO : PROGRESS: at 31.32% examples, 163601 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:15,509 : INFO : PROGRESS: at 31.82% examples, 163676 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:16,534 : INFO : PROGRESS: at 32.27% examples, 163296 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:17,665 : INFO : PROGRESS: at 32.56% examples, 161888 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:10:18,694 : INFO : PROGRESS: at 33.00% examples, 161528 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:19,711 : INFO : PROGRESS: at 33.57% examples, 161768 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:20,725 : INFO : PROGRESS: at 34.11% examples, 161920 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:21,725 : INFO : PROGRESS: at 34.72% examples, 162470 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:22,731 : INFO : PROGRESS: at 35.38% examples, 163160 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:23,760 : INFO : PROGRESS: at 35.97% examples, 163428 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:24,779 : INFO : PROGRESS: at 36.49% examples, 163442 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:25,798 : INFO : PROGRESS: at 36.97% examples, 163282 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:26,808 : INFO : PROGRESS: at 37.47% examples, 163251 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:27,839 : INFO : PROGRESS: at 37.90% examples, 162836 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:28,903 : INFO : PROGRESS: at 38.28% examples, 162192 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:29,915 : INFO : PROGRESS: at 38.83% examples, 162323 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:30,937 : INFO : PROGRESS: at 39.45% examples, 162757 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:31,947 : INFO : PROGRESS: at 40.08% examples, 163272 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:32,955 : INFO : PROGRESS: at 40.78% examples, 164019 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:33,958 : INFO : PROGRESS: at 41.42% examples, 164527 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:34,963 : INFO : PROGRESS: at 42.11% examples, 165164 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:35,993 : INFO : PROGRESS: at 42.71% examples, 165429 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:37,005 : INFO : PROGRESS: at 43.26% examples, 165511 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:38,006 : INFO : PROGRESS: at 43.73% examples, 165315 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:39,012 : INFO : PROGRESS: at 44.22% examples, 165177 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:40,047 : INFO : PROGRESS: at 44.73% examples, 165062 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:10:41,107 : INFO : PROGRESS: at 45.05% examples, 164271 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:10:42,112 : INFO : PROGRESS: at 45.44% examples, 163818 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:43,118 : INFO : PROGRESS: at 45.93% examples, 163711 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:44,141 : INFO : PROGRESS: at 46.58% examples, 164111 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:45,150 : INFO : PROGRESS: at 47.26% examples, 164674 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:46,172 : INFO : PROGRESS: at 47.93% examples, 165205 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:47,177 : INFO : PROGRESS: at 48.53% examples, 165489 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:48,208 : INFO : PROGRESS: at 48.99% examples, 165262 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:49,223 : INFO : PROGRESS: at 49.46% examples, 165074 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:50,273 : INFO : PROGRESS: at 49.97% examples, 165021 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:51,279 : INFO : PROGRESS: at 50.48% examples, 164974 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:52,292 : INFO : PROGRESS: at 50.92% examples, 164737 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:53,327 : INFO : PROGRESS: at 51.38% examples, 164528 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:54,355 : INFO : PROGRESS: at 51.95% examples, 164705 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:55,384 : INFO : PROGRESS: at 52.57% examples, 165002 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:56,384 : INFO : PROGRESS: at 53.15% examples, 165212 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:57,425 : INFO : PROGRESS: at 53.70% examples, 165225 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:58,430 : INFO : PROGRESS: at 54.17% examples, 165130 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:10:59,434 : INFO : PROGRESS: at 54.68% examples, 165096 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:00,467 : INFO : PROGRESS: at 55.24% examples, 165192 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:01,475 : INFO : PROGRESS: at 55.76% examples, 165201 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:02,495 : INFO : PROGRESS: at 56.15% examples, 164801 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:03,501 : INFO : PROGRESS: at 56.75% examples, 165041 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:04,514 : INFO : PROGRESS: at 57.43% examples, 165498 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:05,540 : INFO : PROGRESS: at 58.06% examples, 165818 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:06,540 : INFO : PROGRESS: at 58.76% examples, 166328 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:07,547 : INFO : PROGRESS: at 59.46% examples, 166822 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:08,574 : INFO : PROGRESS: at 60.04% examples, 166952 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:09,612 : INFO : PROGRESS: at 60.60% examples, 167024 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:10,633 : INFO : PROGRESS: at 61.06% examples, 166845 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:11,660 : INFO : PROGRESS: at 61.61% examples, 166865 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:12,662 : INFO : PROGRESS: at 62.14% examples, 166869 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:13,678 : INFO : PROGRESS: at 62.55% examples, 166543 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:14,704 : INFO : PROGRESS: at 63.19% examples, 166827 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:15,719 : INFO : PROGRESS: at 63.90% examples, 167268 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:16,733 : INFO : PROGRESS: at 64.58% examples, 167647 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:17,742 : INFO : PROGRESS: at 65.19% examples, 167834 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:18,760 : INFO : PROGRESS: at 65.80% examples, 168060 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:19,766 : INFO : PROGRESS: at 66.34% examples, 168042 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:20,785 : INFO : PROGRESS: at 66.94% examples, 168208 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:21,800 : INFO : PROGRESS: at 67.46% examples, 168184 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:22,826 : INFO : PROGRESS: at 67.93% examples, 168007 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:23,831 : INFO : PROGRESS: at 68.50% examples, 168139 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:24,865 : INFO : PROGRESS: at 69.03% examples, 168095 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:25,979 : INFO : PROGRESS: at 69.20% examples, 167107 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:26,979 : INFO : PROGRESS: at 69.54% examples, 166697 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:27,980 : INFO : PROGRESS: at 70.10% examples, 166798 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:29,000 : INFO : PROGRESS: at 70.68% examples, 166919 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:30,022 : INFO : PROGRESS: at 71.36% examples, 167264 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:31,029 : INFO : PROGRESS: at 72.00% examples, 167581 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:32,050 : INFO : PROGRESS: at 72.58% examples, 167695 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:33,077 : INFO : PROGRESS: at 72.87% examples, 167131 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:34,102 : INFO : PROGRESS: at 73.11% examples, 166446 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:35,112 : INFO : PROGRESS: at 73.75% examples, 166704 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:36,132 : INFO : PROGRESS: at 74.34% examples, 166865 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:37,137 : INFO : PROGRESS: at 75.00% examples, 167174 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:38,152 : INFO : PROGRESS: at 75.63% examples, 167417 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:39,175 : INFO : PROGRESS: at 76.30% examples, 167692 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:40,186 : INFO : PROGRESS: at 76.92% examples, 167887 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:41,210 : INFO : PROGRESS: at 77.52% examples, 168035 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:42,237 : INFO : PROGRESS: at 78.15% examples, 168255 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:43,297 : INFO : PROGRESS: at 78.62% examples, 168060 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:44,431 : INFO : PROGRESS: at 78.91% examples, 167420 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:45,440 : INFO : PROGRESS: at 79.28% examples, 167087 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:46,445 : INFO : PROGRESS: at 79.84% examples, 167168 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:47,451 : INFO : PROGRESS: at 80.43% examples, 167333 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:48,479 : INFO : PROGRESS: at 81.03% examples, 167466 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:49,496 : INFO : PROGRESS: at 81.60% examples, 167529 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:50,509 : INFO : PROGRESS: at 82.15% examples, 167556 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:51,518 : INFO : PROGRESS: at 82.70% examples, 167582 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:52,543 : INFO : PROGRESS: at 83.35% examples, 167793 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:53,548 : INFO : PROGRESS: at 83.93% examples, 167903 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:54,570 : INFO : PROGRESS: at 84.28% examples, 167529 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:55,662 : INFO : PROGRESS: at 84.52% examples, 166859 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:11:56,685 : INFO : PROGRESS: at 85.06% examples, 166880 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:57,704 : INFO : PROGRESS: at 85.72% examples, 167131 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:58,745 : INFO : PROGRESS: at 86.41% examples, 167390 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:11:59,756 : INFO : PROGRESS: at 87.05% examples, 167604 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:00,767 : INFO : PROGRESS: at 87.55% examples, 167554 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:01,769 : INFO : PROGRESS: at 88.19% examples, 167778 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:12:02,782 : INFO : PROGRESS: at 88.75% examples, 167835 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:03,788 : INFO : PROGRESS: at 89.23% examples, 167761 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:04,832 : INFO : PROGRESS: at 89.61% examples, 167465 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:05,991 : INFO : PROGRESS: at 89.80% examples, 166704 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:12:07,007 : INFO : PROGRESS: at 90.08% examples, 166228 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:12:08,045 : INFO : PROGRESS: at 90.66% examples, 166307 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:09,048 : INFO : PROGRESS: at 91.20% examples, 166350 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:10,077 : INFO : PROGRESS: at 91.84% examples, 166582 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:11,078 : INFO : PROGRESS: at 92.50% examples, 166838 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:12,085 : INFO : PROGRESS: at 93.18% examples, 167117 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:13,104 : INFO : PROGRESS: at 93.83% examples, 167344 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:14,124 : INFO : PROGRESS: at 94.51% examples, 167606 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:15,137 : INFO : PROGRESS: at 95.01% examples, 167562 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:12:16,272 : INFO : PROGRESS: at 95.34% examples, 167103 words/s, in_qsize 2, out_qsize 0\n",
      "2017-03-13 17:12:17,290 : INFO : PROGRESS: at 95.81% examples, 166985 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:18,295 : INFO : PROGRESS: at 96.44% examples, 167190 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:19,321 : INFO : PROGRESS: at 97.12% examples, 167434 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:20,364 : INFO : PROGRESS: at 97.76% examples, 167603 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:21,386 : INFO : PROGRESS: at 98.41% examples, 167816 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:22,413 : INFO : PROGRESS: at 99.11% examples, 168053 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:23,431 : INFO : PROGRESS: at 99.62% examples, 168039 words/s, in_qsize 1, out_qsize 0\n",
      "2017-03-13 17:12:24,081 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-13 17:12:24,081 : INFO : training on 51561405 raw words (32102422 effective words) took 191.0s, 168113 effective words/s\n",
      "2017-03-13 17:12:24,083 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-13 17:12:24,570 : INFO : saving Word2Vec object under 300features_5minwords_20context, separately None\n",
      "2017-03-13 17:12:24,571 : INFO : not storing attribute syn0norm\n",
      "2017-03-13 17:12:24,571 : INFO : not storing attribute cum_table\n",
      "2017-03-13 17:12:28,906 : INFO : saved 300features_5minwords_20context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300      # Word vector dimensionality                      \n",
    "min_word_count = 5      # Minimum word count                        \n",
    "num_workers = 1         # Number of threads to run in parallel\n",
    "context = 20            # Context window size                                                                                    \n",
    "downsampling = 1e-4     # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Initialize and train the model\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers,\n",
    "                          size = num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context, \n",
    "                          sample = downsampling)\n",
    "\n",
    "# Call init_sims because we won't train the model any further \n",
    "# This will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save the model for potential, future use.\n",
    "model_name = \"{}features_{}minwords_{}context\".format(num_features,min_word_count,context)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the model, if necessary\n",
    "# model = Word2Vec.load(\"300features_5minwords_20context\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.6236348748207092), ('crippl', 0.6064074039459229), ('harden', 0.5961687564849854), ('loner', 0.5913522839546204), ('businessman', 0.5873215198516846), ('unwit', 0.5845123529434204), ('rancher', 0.5801037549972534), ('alonzo', 0.5760775804519653), ('deed', 0.5737305879592896), ('greedi', 0.5680551528930664)]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the performance of the model\n",
    "print(model.most_similar(\"man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excel', 0.728122353553772),\n",
       " ('fantast', 0.7253319621086121),\n",
       " ('terrif', 0.6595006585121155),\n",
       " ('outstand', 0.6500605344772339),\n",
       " ('superb', 0.6353731751441956),\n",
       " ('brilliant', 0.6136918663978577),\n",
       " ('fine', 0.5936250686645508),\n",
       " ('good', 0.5925694704055786),\n",
       " ('awesom', 0.5890749096870422),\n",
       " ('amaz', 0.584725022315979)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horribl', 0.8962301015853882),\n",
       " ('aw', 0.8750690221786499),\n",
       " ('atroci', 0.8494848012924194),\n",
       " ('horrend', 0.8438228964805603),\n",
       " ('horrid', 0.8207361698150635),\n",
       " ('lousi', 0.7899401783943176),\n",
       " ('abysm', 0.7819921970367432),\n",
       " ('bad', 0.7641528844833374),\n",
       " ('laughabl', 0.7585679888725281),\n",
       " ('amateurish', 0.738487958908081)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"terribl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.6199691891670227),\n",
       " ('film', 0.607010006904602),\n",
       " ('flick', 0.5626986026763916),\n",
       " ('honest', 0.5343186855316162),\n",
       " ('realli', 0.5284943580627441),\n",
       " ('probabl', 0.5283719897270203),\n",
       " ('blockbust', 0.514778733253479),\n",
       " ('those', 0.5122922658920288),\n",
       " ('watcher', 0.5061703324317932),\n",
       " ('actual', 0.5050208568572998)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"movi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finest', 0.7128208875656128),\n",
       " ('greatest', 0.6819601655006409),\n",
       " ('favourit', 0.6107134819030762),\n",
       " ('funniest', 0.6005286574363708),\n",
       " ('arguabl', 0.5901353359222412),\n",
       " ('weakest', 0.5900111794471741),\n",
       " ('underr', 0.5632859468460083),\n",
       " ('favorit', 0.5546506643295288),\n",
       " ('undoubt', 0.5520817041397095),\n",
       " ('worst', 0.5461603403091431)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model looks good so far. Each of these words has appropriate similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 17:14:22,514 : WARNING : direct access to syn0 will not be supported in future gensim releases, please use model.wv.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32269, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 6549.41\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 6407.38\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 6368.37\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 6352.34\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 6345.52\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 6341.61\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 6339.4\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 6337.72\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 6336.79\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 6336.27\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 6336.03\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 6335.88\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 6335.67\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 6335.56\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 6335.52\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 6335.51\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 6335.51\n",
      "center shift 0.000000e+00 within tolerance 2.242093e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 6542.44\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 6399.44\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 6359.77\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 6344.6\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 6337.82\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 6334.31\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 6332.7\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 6331.39\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 6330.64\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 6330.25\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 6330.06\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 6329.92\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 6329.86\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 6329.84\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 6329.84\n",
      "center shift 0.000000e+00 within tolerance 2.242093e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 6549.91\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 6408.76\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 6372.29\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 6357.28\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 6349.68\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 6345.21\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 6342.71\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 6341.28\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 6340.35\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 6339.95\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 6339.82\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 6339.71\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 6339.65\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 6339.58\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 6339.52\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 6339.52\n",
      "center shift 0.000000e+00 within tolerance 2.242093e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 6547.91\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 6410.07\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 6372.04\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 6356.46\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 6349.3\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 6345.69\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 6343.59\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 6342.24\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 6341.47\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 6341.09\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 6340.78\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 6340.54\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 6340.5\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 6340.43\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 14, inertia 6340.39\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 15, inertia 6340.37\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 16, inertia 6340.37\n",
      "center shift 0.000000e+00 within tolerance 2.242093e-07\n",
      "Initialization complete\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 0, inertia 6550.04\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 1, inertia 6406.62\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 2, inertia 6367.84\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 3, inertia 6350.85\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 4, inertia 6343.14\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 5, inertia 6338.75\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 6, inertia 6336.49\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 7, inertia 6335.43\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 8, inertia 6334.85\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 9, inertia 6334.53\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 10, inertia 6334.32\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 11, inertia 6334.16\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 12, inertia 6334.12\n",
      "start iteration\n",
      "done sorting\n",
      "end inner loop\n",
      "Iteration 13, inertia 6334.12\n",
      "center shift 0.000000e+00 within tolerance 2.242093e-07\n"
     ]
    }
   ],
   "source": [
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0 \n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters,\n",
    "                           n_init = 5,\n",
    "                           verbose = 2)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews are clean\n",
      "Unlabeled training reviews are clean\n",
      "Testing reviews are clean\n"
     ]
    }
   ],
   "source": [
    "# Clean the training and testing reviews.\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review))\n",
    "    \n",
    "print(\"Training reviews are clean\")  \n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review))\n",
    "    \n",
    "print(\"Testing reviews are clean\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'stuff',\n",
       " 'go',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'i',\n",
       " 'have',\n",
       " 'start',\n",
       " 'listen',\n",
       " 'music',\n",
       " 'watch',\n",
       " 'odd',\n",
       " 'documentari',\n",
       " 'there',\n",
       " 'watch',\n",
       " 'wiz',\n",
       " 'watch',\n",
       " 'moonwalk',\n",
       " 'again',\n",
       " 'mayb',\n",
       " 'want',\n",
       " 'get',\n",
       " 'certain',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'thought',\n",
       " 'realli',\n",
       " 'cool',\n",
       " 'eighti',\n",
       " 'mayb',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'whether',\n",
       " 'guilti',\n",
       " 'innoc',\n",
       " 'moonwalk',\n",
       " 'part',\n",
       " 'biographi',\n",
       " 'part',\n",
       " 'featur',\n",
       " 'film',\n",
       " 'rememb',\n",
       " 'go',\n",
       " 'see',\n",
       " 'cinema',\n",
       " 'origin',\n",
       " 'releas',\n",
       " 'subtl',\n",
       " 'messag',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'feel',\n",
       " 'toward',\n",
       " 'press',\n",
       " 'also',\n",
       " 'obvious',\n",
       " 'messag',\n",
       " 'drug',\n",
       " 'bad',\n",
       " \"m'kay\",\n",
       " 'visual',\n",
       " 'impress',\n",
       " 'cours',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'unless',\n",
       " 'remot',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'anyway',\n",
       " 'go',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'bore',\n",
       " 'may',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consent',\n",
       " 'make',\n",
       " 'movi',\n",
       " 'mj',\n",
       " 'fan',\n",
       " 'would',\n",
       " 'say',\n",
       " 'made',\n",
       " 'fan',\n",
       " 'true',\n",
       " 'realli',\n",
       " 'nice',\n",
       " 'him',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'featur',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'final',\n",
       " 'start',\n",
       " '20',\n",
       " 'minut',\n",
       " 'exclud',\n",
       " 'smooth',\n",
       " 'crimin',\n",
       " 'sequenc',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convinc',\n",
       " 'psychopath',\n",
       " 'power',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'want',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'beyond',\n",
       " 'me',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plan',\n",
       " '?',\n",
       " 'nah',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " \"'s\",\n",
       " 'charact',\n",
       " 'rant',\n",
       " 'want',\n",
       " 'peopl',\n",
       " 'know',\n",
       " 'suppli',\n",
       " 'drug',\n",
       " 'etc',\n",
       " 'dunno',\n",
       " 'mayb',\n",
       " 'hate',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'music',\n",
       " 'lot',\n",
       " 'cool',\n",
       " 'thing',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'turn',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'whole',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequenc',\n",
       " 'also',\n",
       " 'director',\n",
       " 'must',\n",
       " 'patienc',\n",
       " 'saint',\n",
       " 'came',\n",
       " 'film',\n",
       " 'kiddi',\n",
       " 'bad',\n",
       " 'sequenc',\n",
       " 'usual',\n",
       " 'director',\n",
       " 'hate',\n",
       " 'work',\n",
       " 'one',\n",
       " 'kid',\n",
       " 'let',\n",
       " 'alon',\n",
       " 'whole',\n",
       " 'bunch',\n",
       " 'perform',\n",
       " 'complex',\n",
       " 'danc',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movi',\n",
       " 'peopl',\n",
       " 'like',\n",
       " 'mj',\n",
       " 'one',\n",
       " 'level',\n",
       " 'anoth',\n",
       " 'which',\n",
       " 'think',\n",
       " 'peopl',\n",
       " 'not',\n",
       " 'stay',\n",
       " 'away',\n",
       " 'tri',\n",
       " 'give',\n",
       " 'wholesom',\n",
       " 'messag',\n",
       " 'iron',\n",
       " 'mj',\n",
       " \"'s\",\n",
       " 'bestest',\n",
       " 'buddi',\n",
       " 'movi',\n",
       " 'girl',\n",
       " '!',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'truli',\n",
       " 'one',\n",
       " 'talent',\n",
       " 'peopl',\n",
       " 'ever',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilti',\n",
       " '?',\n",
       " 'well',\n",
       " 'attent',\n",
       " 'i',\n",
       " 'have',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'well',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'peopl',\n",
       " 'differ',\n",
       " 'behind',\n",
       " 'close',\n",
       " 'door',\n",
       " 'know',\n",
       " 'fact',\n",
       " 'either',\n",
       " 'extrem',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'sickest',\n",
       " 'liar',\n",
       " 'hope',\n",
       " 'latter']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur',\n",
       " 'film',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'main',\n",
       " 'theme',\n",
       " 'mortal',\n",
       " 'nostalgia',\n",
       " 'loss',\n",
       " 'innoc',\n",
       " 'perhap',\n",
       " 'surpris',\n",
       " 'rate',\n",
       " 'high',\n",
       " 'older',\n",
       " 'viewer',\n",
       " 'younger',\n",
       " 'one',\n",
       " 'howev',\n",
       " 'craftsmanship',\n",
       " 'complet',\n",
       " 'film',\n",
       " 'anyon',\n",
       " 'enjoy',\n",
       " 'pace',\n",
       " 'steadi',\n",
       " 'constant',\n",
       " 'charact',\n",
       " 'full',\n",
       " 'engag',\n",
       " 'relationship',\n",
       " 'interact',\n",
       " 'natur',\n",
       " 'show',\n",
       " 'need',\n",
       " 'flood',\n",
       " 'tear',\n",
       " 'show',\n",
       " 'emot',\n",
       " 'scream',\n",
       " 'show',\n",
       " 'fear',\n",
       " 'shout',\n",
       " 'show',\n",
       " 'disput',\n",
       " 'violenc',\n",
       " 'show',\n",
       " 'anger',\n",
       " 'natur',\n",
       " 'joyc',\n",
       " \"'s\",\n",
       " 'short',\n",
       " 'stori',\n",
       " 'lend',\n",
       " 'film',\n",
       " 'readi',\n",
       " 'made',\n",
       " 'structur',\n",
       " 'perfect',\n",
       " 'polish',\n",
       " 'diamond',\n",
       " 'small',\n",
       " 'chang',\n",
       " 'huston',\n",
       " 'make',\n",
       " 'inclus',\n",
       " 'poem',\n",
       " 'fit',\n",
       " 'neat',\n",
       " 'truli',\n",
       " 'masterpiec',\n",
       " 'tact',\n",
       " 'subtleti',\n",
       " 'overwhelm',\n",
       " 'beauti']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    \n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    \n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews are complete.\n",
      "Testing reviews are complete.\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Training reviews are complete.\")    \n",
    "    \n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map )\n",
    "    counter += 1\n",
    "    \n",
    "print(\"Testing reviews are complete.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use GridSearchCV to find the optimal parameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_GridSearch(model, model_paramters, x_values):\n",
    "    '''Find the optimal parameters for a model'''\n",
    "    grid = GridSearchCV(model, model_paramters, scoring = 'roc_auc')\n",
    "    grid.fit(x_values, train.sentiment)\n",
    "\n",
    "    print(\"Best grid score = \", grid.best_score_)\n",
    "    print(\"Best Parameters = \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.92957452917\n",
      "Best Parameters =  {'n_estimators': 300, 'min_samples_leaf': 1, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# RandomForect Classifier\n",
    "rfc_parameters = {'n_estimators':[100,200,300],\n",
    "                  'max_depth':[3,5,7,None],\n",
    "                  'min_samples_leaf': [1,2,3]}\n",
    "\n",
    "rfc_model = rfc()\n",
    "\n",
    "use_GridSearch(rfc_model, rfc_parameters, train_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.942853192921\n",
      "Best Parameters =  {'max_iter': 5, 'C': 0.01, 'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr_parameters = {'C':[0.005,0.01,0.05],\n",
    "                 'max_iter':[4,5,6],\n",
    "                 'fit_intercept': [True]}\n",
    "\n",
    "lr_model = lr()\n",
    "\n",
    "use_GridSearch(lr_model, lr_parameters, train_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.92408550165\n",
      "Best Parameters =  {'penalty': 'l1', 'loss': 'log'}\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent Classifier \n",
    "sgd_parameters = {'loss': ['log'],\n",
    "                  'penalty': ['l1','l2','none']}\n",
    "\n",
    "sgd_model = sgd()\n",
    "\n",
    "use_GridSearch(sgd_model, sgd_parameters, train_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the quality of the classifiers with cross validation, then train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use_model(model, x_values):\n",
    "    '''\n",
    "    Test the quality of a model using cross validation\n",
    "    Train the model with the x_values\n",
    "    '''\n",
    "    scores = cross_val_score(model, x_values, train.sentiment, cv = 5, scoring = 'roc_auc')\n",
    "    model.fit(x_values, train.sentiment)\n",
    "    mean_score = round(np.mean(scores) * 100,2) \n",
    "\n",
    "    print(scores)\n",
    "    print()\n",
    "    print(\"Mean score = {}\".format(mean_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92891048  0.9312636   0.93165904  0.93291288  0.92610464]\n",
      "\n",
      "Mean score = 93.02\n"
     ]
    }
   ],
   "source": [
    "rfc_model = rfc(n_estimators = 300,\n",
    "                max_depth = None,\n",
    "                min_samples_leaf = 1)\n",
    "\n",
    "use_model(rfc_model, train_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94450368  0.94777888  0.94073952  0.94730352  0.9427424 ]\n",
      "\n",
      "Mean score = 94.46\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr(C = 0.01,\n",
    "              max_iter = 5,\n",
    "              fit_intercept = True)\n",
    "\n",
    "use_model(lr_model, train_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92591808  0.92870432  0.91723472  0.92715936  0.92216176]\n",
      "\n",
      "Mean score = 92.42\n"
     ]
    }
   ],
   "source": [
    "sgd_model = sgd(loss = 'log',\n",
    "                penalty = 'l1')\n",
    "\n",
    "use_model(sgd_model, train_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_result = rfc_model.predict(test_centroids)\n",
    "lr_result = lr_model.predict(test_centroids)\n",
    "sgd_result = sgd_model.predict(test_centroids)\n",
    "\n",
    "avg_result = (lr_result + rfc_result + sgd_result) / 3\n",
    "\n",
    "avg_result_final = []\n",
    "for result in avg_result:\n",
    "    if result > 0.5:\n",
    "        avg_result_final.append(1)\n",
    "    else:\n",
    "        avg_result_final.append(0)\n",
    "        \n",
    "avg_output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":avg_result_final})\n",
    "avg_output.to_csv(\"avg_centroids_submission.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1\n",
       "5    \"2913_8\"          1\n",
       "6    \"4396_1\"          0\n",
       "7     \"395_2\"          1\n",
       "8   \"10616_1\"          0\n",
       "9    \"9074_9\"          0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the submission file\n",
    "avg_output[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I submit the results to the Kaggle competition its score (area under the ROC curve) is 0.880, which ranks 266/578, top 46%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49577\n"
     ]
    }
   ],
   "source": [
    "# Count the number of different words in the reviews\n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "for comment in clean_train_reviews:\n",
    "    word_counts[\" \".join(comment)] += 1\n",
    "\n",
    "for comment in clean_test_reviews:\n",
    "    word_counts[\" \".join(comment)] += 1\n",
    "print(len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the parameters for vectorizing the words in the reviews.\n",
    "vectorizer = TfidfVectorizer(max_features = len(word_counts), \n",
    "                             ngram_range = (1, 3), \n",
    "                             sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the words of the reviews.\n",
    "# The list of lists becomes just a list of strings (strings = reviews).\n",
    "clean_train_reviews_join = []\n",
    "for review in clean_train_reviews:\n",
    "    clean_train_reviews_join.append(\" \".join(review))\n",
    "\n",
    "clean_test_reviews_join = []\n",
    "for review in clean_test_reviews:\n",
    "    clean_test_reviews_join.append(\" \".join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_vec is complete.\n",
      "x_test_vec is complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the vectorizer on the vocabulary and convert reviews into matrices.\n",
    "x_train_vec = vectorizer.fit_transform(clean_train_reviews_join)\n",
    "print(\"x_train_vec is complete.\")\n",
    "x_test_vec = vectorizer.transform(clean_test_reviews_join)\n",
    "print(\"x_test_vec is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GridSearchcv to find the best parameters, just like with Method 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.933537007858\n",
      "Best Parameters =  {'n_estimators': 200, 'min_samples_leaf': 3, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "rfc_parameters_vec = {'n_estimators':[100,200,300],\n",
    "                      'max_depth':[3,5,7,None],\n",
    "                      'min_samples_leaf': [1,2,3,4]}\n",
    "\n",
    "rfc_model_vec = rfc()\n",
    "\n",
    "use_GridSearch(rfc_model_vec, rfc_parameters_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.962700581661\n",
      "Best Parameters =  {'max_iter': 2, 'C': 6, 'fit_intercept': False}\n"
     ]
    }
   ],
   "source": [
    "lr_parameters_vec = {'C':[5,6,7],\n",
    "                 'max_iter':[1,2,3],\n",
    "                 'fit_intercept': [True,False]}\n",
    "\n",
    "lr_model_vec = lr()\n",
    "\n",
    "use_GridSearch(lr_model_vec, lr_parameters_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score =  0.958539153016\n",
      "Best Parameters =  {'penalty': 'none', 'loss': 'log'}\n"
     ]
    }
   ],
   "source": [
    "sgd_parameters_vec = {'loss': ['log'],\n",
    "                  'penalty': ['l1','l2','none']}\n",
    "\n",
    "sgd_model_vec = sgd()\n",
    "\n",
    "use_GridSearch(sgd_model_vec, sgd_parameters_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the quality of the classifiers with cross validation, then train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93125968  0.93375808  0.93373184  0.93458096  0.93305328]\n",
      "\n",
      "Mean score = 93.33\n"
     ]
    }
   ],
   "source": [
    "rfc_model_vec = rfc(n_estimators = 200,\n",
    "                max_depth = None,\n",
    "                min_samples_leaf = 3)\n",
    "\n",
    "use_model(rfc_model_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96271008  0.96529376  0.95973088  0.96456848  0.96171536]\n",
      "\n",
      "Mean score = 96.28\n"
     ]
    }
   ],
   "source": [
    "lr_model_vec = lr(C = 6,\n",
    "              max_iter = 2,\n",
    "              fit_intercept = False)\n",
    "\n",
    "use_model(lr_model_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95948544  0.96322432  0.9573232   0.96245712  0.95898608]\n",
      "\n",
      "Mean score = 96.03\n"
     ]
    }
   ],
   "source": [
    "sgd_model_vec = sgd(loss = 'log',\n",
    "                penalty = 'none')\n",
    "\n",
    "use_model(sgd_model_vec, x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_result_vec = lr_model_vec.predict(x_test_vec)\n",
    "rfc_result_vec = rfc_model_vec.predict(x_test_vec)\n",
    "sgd_result_vec = sgd_model_vec.predict(x_test_vec)\n",
    "\n",
    "avg_result_vec = (lr_result_vec + rfc_result_vec + sgd_result_vec) / 3\n",
    "\n",
    "avg_result_final_vec = []\n",
    "for result in avg_result_vec:\n",
    "    if result > 0.5:\n",
    "        avg_result_final_vec.append(1)\n",
    "    else:\n",
    "        avg_result_final_vec.append(0)\n",
    "        \n",
    "avg_output_vec = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":avg_result_final_vec})\n",
    "avg_output_vec.to_csv(\"avg_vec_submission1.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1\n",
       "5    \"2913_8\"          1\n",
       "6    \"4396_1\"          0\n",
       "7     \"395_2\"          1\n",
       "8   \"10616_1\"          0\n",
       "9    \"9074_9\"          0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_output_vec[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method scores slightly higher, 0.895, which ranks 251/578, top 44%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see what happens when we combine all six predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_result_combine = (lr_result + rfc_result + sgd_result +\n",
    "                      lr_result_vec + rfc_result_vec + sgd_result_vec) / 6\n",
    "\n",
    "avg_result_final_combine = []\n",
    "for result in avg_result_combine:\n",
    "    if result > 0.5:\n",
    "        avg_result_final_combine.append(1)\n",
    "    else:\n",
    "        avg_result_final_combine.append(0)\n",
    "        \n",
    "avg_output_combine = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":avg_result_final_combine})\n",
    "avg_output_combine.to_csv(\"avg_combine_submission.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"2913_8\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"4396_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"395_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"10616_1\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"9074_9\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1\n",
       "5    \"2913_8\"          1\n",
       "6    \"4396_1\"          0\n",
       "7     \"395_2\"          1\n",
       "8   \"10616_1\"          0\n",
       "9    \"9074_9\"          0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_output_combine[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'combined' submission scored inbetween method 1 and 2, 0.892. I expected that this ensemble strategy would have scored better than the two previous methods, but unfortunately, it did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing tutorial from Google scores 0.845, which is the \"Word2Vec - Bag of Centroids\" example. I am pleased that I have improved upon this example, and built another method that scores even higher. Although I did not score at the top of the leaderboard, I am still pleased with my results and have learned a great deal. One thing that I will focus on with further projects of this nature is reducing the amount of overfitting. As you probably noticed, my models perform much better on the training data than the testing data. If I find some useful strategies online for producing models that generalize better, I'll try to return to this code to improve its results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
